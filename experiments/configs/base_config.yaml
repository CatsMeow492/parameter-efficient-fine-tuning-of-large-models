# Base LoRA Configuration for Baseline Experiments
# Standard settings based on LoRA paper recommendations

model:
  name: "microsoft/DialoGPT-medium"  # Start with smaller model for initial testing
  # name: "meta-llama/Llama-2-7b-hf"  # Upgrade to this once baseline works
  cache_dir: "./models"
  
lora:
  r: 16                    # LoRA rank (standard setting)
  lora_alpha: 32          # LoRA scaling factor (2 * r is common)
  lora_dropout: 0.1       # Dropout for LoRA layers
  bias: "none"            # Don't adapt bias terms
  task_type: "CAUSAL_LM"  # Causal language modeling
  
  # Target modules for standard LoRA (all linear layers)
  target_modules:
    - "c_attn"      # Attention weights (for DialoGPT)
    - "c_proj"      # Attention projection
    - "c_fc"        # Feed-forward layer 1
    # For LLaMA, use: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

training:
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # Memory optimization
  fp16: true
  gradient_checkpointing: true
  dataloader_pin_memory: false
  
  # Evaluation and logging
  evaluation_strategy: "steps"
  eval_steps: 100
  logging_steps: 50
  save_steps: 500
  save_total_limit: 2
  
data:
  dataset_name: "tatsu-lab/alpaca"
  max_length: 512
  train_split: "train[:1000]"  # Small subset for initial testing
  eval_split: "train[1000:1200]"  # Small eval set
  
  # Data preprocessing
  instruction_template: "### Instruction:\n{instruction}\n\n### Response:\n{output}"
  response_template: "### Response:"

experiment:
  name: "baseline_lora"
  output_dir: "./results/baseline_lora"
  logging_dir: "./results/baseline_lora/logs"
  seed: 42
  
  # Weights & Biases (optional)
  wandb_project: "lora-placement-study"
  wandb_run_name: "baseline-standard-lora" 