# Feed-Forward Only LoRA Configuration
# Testing: Does FF-only placement achieve similar results with fewer parameters?

model:
  name: "microsoft/DialoGPT-medium"
  cache_dir: "./models"
  
lora:
  r: 16                    # Same rank as baseline for fair comparison
  lora_alpha: 32          # Same alpha as baseline
  lora_dropout: 0.1       # Same dropout as baseline
  bias: "none"            
  task_type: "CAUSAL_LM"  
  
  # Target modules: ONLY feed-forward layers (hypothesis test)
  target_modules:
    - "c_fc"              # Feed-forward layer only
    # REMOVED: "c_attn", "c_proj" (attention layers)

training:
  num_train_epochs: 3
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.01
  
  # Memory optimization
  fp16: false               
  gradient_checkpointing: false  
  dataloader_pin_memory: false
  
  # Evaluation and logging
  evaluation_strategy: "steps"
  eval_steps: 100
  logging_steps: 50
  save_steps: 500
  save_total_limit: 2
  
data:
  dataset_name: "tatsu-lab/alpaca"
  max_length: 256
  train_split: "train[:100]"  # Same dataset for fair comparison
  eval_split: "train[100:120]"
  
  # Data preprocessing
  instruction_template: "### Instruction:\n{instruction}\n\n### Response:\n{output}"
  response_template: "### Response:"

experiment:
  name: "ff_only_lora"
  output_dir: "./results/ff_only_lora"
  logging_dir: "./results/ff_only_lora/logs"
  seed: 42
  
  # Weights & Biases
  wandb_project: "lora-placement-study"
  wandb_run_name: "ff-only-lora" 